# -*- coding: utf-8 -*-
"""Speech recogniton.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HSdDDCyQFkg2B20XYPACxIm9BuOh3kiU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from scipy.io import wavfile
import os.path
import IPython.display
import seaborn as sns
import librosa
import librosa.display
import soundfile

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import confusion_matrix

import tensorflow as tf
from tensorflow.keras import utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten, BatchNormalization
from keras import optimizers

import warnings

from google.colab import drive
drive.mount('/content/drive')

recFile = Path("./drive/MyDrive/Crema 2")

filepath = list(recFile.glob(r'**/*.wav'))

labels = list(map(lambda x: os.path.split(x)[1].split('_')[2], filepath))

set(labels)

filepath = pd.Series(filepath, name='Filepath').astype(str)
labels = pd.Series(labels, name='Label')

audio_df = pd.concat([filepath, labels], axis=1)
audio_df

# show the distribution of the different emotions
sns.set(rc={'figure.figsize':(12,8)})
sns.set_style('darkgrid')
sns.histplot(labels, color='#4FAEB0')

audio_arrays = []

for i in audio_df['Filepath']:
    x, sr = librosa.load(i, sr=44100)
    audio_arrays.append(x)

audio_df['Arrays'] = audio_arrays

audio_df

audio_df['Arrays'].head()

angfile = audio_df[audio_df['Label'] == 'ANG']['Filepath']
angarray = audio_df[audio_df['Label'] == 'ANG']['Arrays']

librosa.display.waveshow(angarray.iloc[0], color='#C00808')
IPython.display.Audio(angfile.iloc[0])

feafile = audio_df[audio_df['Label'] == 'FEA']['Filepath']
feaarray = audio_df[audio_df['Label'] == 'FEA']['Arrays']

librosa.display.waveshow(feaarray.iloc[0], color='#7D55AA')
IPython.display.Audio(feafile.iloc[0])

hapfile = audio_df[audio_df['Label'] == 'HAP']['Filepath']
haparray = audio_df[audio_df['Label'] == 'HAP']['Arrays']

librosa.display.waveshow(haparray.iloc[0], color='#F19C0E')
IPython.display.Audio(hapfile.iloc[0])

neufile = audio_df[audio_df['Label'] == 'NEU']['Filepath']
neuarray = audio_df[audio_df['Label'] == 'NEU']['Arrays']

librosa.display.waveshow(neuarray.iloc[0], color='#4CB847')
IPython.display.Audio(neufile.iloc[0])

def noise(data):
    noise_amp = 0.035*np.random.uniform()*np.amax(data)
    data = data + noise_amp*np.random.normal(size=data.shape[0])
    return data

def stretch(data, rate=0.8):
    return librosa.effects.time_stretch(data, rate)

def pitch(data, sampling_rate, pitch_factor=0.7):
    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)

def extract_features(data):
    # Zero Crossing Rate
    result = np.array([])
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)
    result=np.hstack((result, zcr))

    # Chroma_stft
    stft = np.abs(librosa.stft(data))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr, n_fft=200).T, axis=0)
    result = np.hstack((result, chroma_stft))

    # MFCC
    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sr, n_fft=200).T, axis=0)
    result = np.hstack((result, mfcc))

    # MelSpectogram
    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sr, n_fft=200).T, axis=0)
    result = np.hstack((result, mel))

    # Tonnetz
    tonnetz = np.mean(librosa.feature.tonnetz(y=data, sr=sr).T, axis=0);
    result = np.hstack((result, tonnetz));

    return result

def get_features(data):
    result = []

    # without augmentation
    res1 = extract_features(data)
    result.append(res1)

    # with noise
    noise_data = noise(data)
    res2 = extract_features(noise_data)
    result.append(res2)

    # with stretching and pitching
    new_data = stretch(data)
    data_stretch_pitch = pitch(new_data, sr)
    res3 = extract_features(data_stretch_pitch)
    result.append(res3)

    return result

warnings.filterwarnings('ignore')

x = []
y = []
for i in range(len(audio_df)):
    feature=get_features(audio_df['Arrays'].iloc[i]);
    for j in feature:
        x.append(j)
        y.append(audio_df['Label'].iloc[i])

le = LabelEncoder()
y = utils.to_categorical(le.fit_transform(y))
y

x_train, x_test, y_train, y_test = train_test_split(np.array(x), np.array(y), test_size=0.1)

print((x_train.shape, y_train.shape, x_test.shape, y_test.shape))

x_train = np.expand_dims(x_train,axis=2)
x_test = np.expand_dims(x_test,axis=2)

print((x_train.shape, y_train.shape, x_test.shape, y_test.shape))

from tensorflow.keras import optimizers

model = Sequential()
model.add(Conv1D(128, 3, activation='relu', input_shape=(x_train.shape[1], 1)))
model.add(MaxPooling1D((1)))
model.add(Conv1D(256, 3, activation='relu'))
model.add(MaxPooling1D((1)))
model.add(Conv1D(512, 3, activation='relu'))
model.add(MaxPooling1D((1)))
model.add(Conv1D(1024, 3, activation='relu'))
model.add(MaxPooling1D((1)))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy',
             optimizer=optimizers.RMSprop(lr=0.0005),
             metrics=['accuracy'])

#model.compile(optimizer=optimizers.RMSprop(lr=0.0005), loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])

model.summary()

history = model.fit(x_train, y_train,
                    epochs=50,
                    batch_size=128,
                    validation_data=(x_test, y_test))

y_pred = model.predict(x_test)
matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))
ax = sns.heatmap(matrix, annot=True, fmt="d", cmap = 'rocket_r', xticklabels = ['Anger', 'Fear', 'Happiness', 'Neutral'], yticklabels = ['Anger', 'Fear', 'Happiness', 'Neutral',])

model.save('drive/MyDrive/ser_model1')

model.save('drive/MyDrive/ser_model.h5')

new_model = tf.keras.models.load_model('my_model.h5')